# Iterative Refinement System - Complete Flow

## Overview

The system now implements a comprehensive human-in-the-loop learning workflow with:
1. **Retrieval-First Design** - Check memory before generating
2. **Multi-Cycle Code Generation** - Review â†’ Execute â†’ Fix â†’ Retry (up to 3 cycles)
3. **Human Approval Gate** - Only validated experiments enter knowledge base
4. **Deduplication** - Avoid storing redundant similar experiments

---

## 1. Design Generation Flow

### A. Retrieval-First Workflow

```
User Query â†’ Memory Search
              â†“
         Found Similar? (similarity > 0.85)
         â†“YES              â†“NO
    Return Existing    Generate New
         â†“
    Show in UI with metadata:
    - Similarity score
    - Original query
    - Human approved?
    - Past verdict & confidence
         â†“
    User Choice:
    â”œâ”€ Accept â†’ Use retrieved design
    â””â”€ "Generate New" â†’ Force generation (skip retrieval)
```

**Code Location**: `llm_designer.py` lines 290-340

**Key Logic**:
```python
# Check memory for existing experiments
similar_experiments = memory.retrieve_similar_experiments(query, n_results=1)
if similarity > 0.85:
    # Return existing design with metadata
    return convert_stored_to_optical_setup(stored_data)
else:
    # Proceed with generation using memory as context
    enhanced_query = memory.augment_prompt_with_memory(query)
```

### B. Generation with Memory Context

If no high-similarity match found, system enhances prompt with:
- Top 3 similar past experiments (context)
- Top 3 relevant building blocks (patterns)
- Past successes and failures

```
Query Enhancement:
â”œâ”€ Original: "design hong ou mandel setup"
â””â”€ Enhanced: [original] + 
             [3 similar HOM experiments] +
             [beam splitter patterns] +
             [detector configurations]
```

**Code Location**: `memory_system.py` `augment_prompt_with_memory()`

---

## 2. Simulation Validation Flow

### Multi-Cycle Refinement Process

```
Generated Design â†’ Simulation Cycle (max 3 iterations)
                        â†“
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚ Cycle Start  â”‚
                   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚ 1. Code Review   â”‚ â† LLM reviews physics
                   â”‚    (Pre-exec)    â”‚   before execution
                   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
                   Review Pass?
                   â†“NO        â†“YES
             Revise Code   Execute
                   â†“          â†“
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                              â†“
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚ 2. Execution     â”‚
                   â”‚    (Safe sandbox)â”‚
                   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
                   Success?
                   â†“NO        â†“YES
                   â”‚          â†“
                   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚   â”‚ 3. Physics Checkâ”‚
                   â”‚   â”‚    (Validate)   â”‚
                   â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚          â†“
                   â”‚   All metrics valid?
                   â”‚   (no NaN, negative, >1)
                   â”‚   â†“NO        â†“YES
                   â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“          â†“
                  Retry with  Success!
                  error msg   â†“
                       â†“      Return results
                  Next cycle
                  (max 3)
```

**Code Location**: `simulation_agent.py` `_validate_with_code_generation()` lines 145-250

### Key Validation Steps

#### Step 1: Pre-Execution Code Review
```python
review_passed, feedback = _review_simulation_code(design, sim_code)
if not review_passed:
    # Regenerate with review feedback
    sim_code_revised = _generate_simulation_code_with_error(
        design, sim_code, f"Code review: {feedback}"
    )
```

**Checks**:
- Correct quantum operations for experiment type
- Proper state normalization
- Appropriate metric calculations
- Physics consistency (unitarity, conservation)

#### Step 2: Execution with Error Capture
```python
exec_success, results, error_msg = _execute_simulation(sim_code)
```

**Safe sandbox execution**:
- Isolated namespace
- Timeout protection
- Exception capture with full traceback
- Result validation

#### Step 3: Physics Validation
```python
physics_valid, physics_error = _validate_physics(results)
```

**Checks**:
- All variances â‰¥ 0
- All entropies â‰¥ 0
- All purities âˆˆ [0, 1]
- All fidelities âˆˆ [0, 1]
- No NaN or Inf values
- All metrics are real numbers

#### Step 4: Error-Driven Retry
```python
while not exec_success and retry_count < max_retries:
    sim_code_retry = _generate_simulation_code_with_error(
        design, 
        failed_code=sim_code,
        error_msg=error_msg  # Full traceback + physics errors
    )
    exec_success, results, error_msg = _execute_simulation(sim_code_retry)
```

**Error feedback includes**:
- Python exception type and message
- Line number where error occurred
- Full traceback
- Physics violation details (if applicable)
- Suggested fixes based on error type

---

## 3. Simulation Code Generation Prompts

### A. Initial Generation Prompt

**Key Sections** (see `simulation_agent.py` lines 250-400):

```
PART 1: UNDERSTAND DESIGNER'S INTENT
- What are they trying to achieve?
- What components did they specify?
- What parameters did they choose?

PART 2: IMPLEMENT FAITHFULLY
- Extract parameters from components
- Map optical elements to quantum operations
- Use THEIR sequence and parameters exactly

PART 3: VALIDATE IMPLEMENTATION
âœ“ Conservation laws (photon number, trace=1)
âœ“ Mathematical consistency (all positive metrics)
âœ“ Numerical hygiene (normalize, use abs())

PART 4: WORKING EXAMPLE
[Complete Mach-Zehnder code example]

PART 5: CODE STRUCTURE
[Template with exact format expected]
```

**Critical Instructions**:
- **DO NOT redesign** - implement exactly what designer specified
- Normalize after every operation: `state = state.unit()`
- All metrics must be real positive floats
- Visibility requires multiple phases (not single measurement)
- Use proper tensor products for multi-mode states

### B. Error Retry Prompt

**Structure** (see `simulation_agent.py` lines 726-850):

```
DEBUGGING PROTOCOL: ANALYZE BEFORE FIXING

1. IDENTIFY ROOT CAUSE
   - Syntax error?
   - Dimension mismatch?
   - Type error?
   - Physics violation?

2. TRACE THE PHYSICS
   - Which state caused error?
   - Which operation failed?
   - What were you computing?

3. COMMON MISTAKES TO CHECK
   âŒ Forgot normalization
   âŒ Complex treated as real
   âŒ Dimension mismatch
   âŒ Photon number exceeds cutoff
   âŒ Invalid variance calculation
   âŒ Entropy with negative eigenvalues
   âŒ Non-unitary operator

4. PHYSICS VALIDATION CHECKLIST
   âœ“ States normalized
   âœ“ Density matrices trace=1
   âœ“ All variances â‰¥ 0
   âœ“ All entropies â‰¥ 0
   âœ“ All purities âˆˆ [0,1]
   âœ“ Conservation laws respected
```

**Error-Specific Guidance**:
- For dimension errors â†’ Check tensor product dimensions
- For negative values â†’ Use `abs()` or validate calculation
- For NaN â†’ Check for log(0) or divide by zero
- For complex â†’ Use `.real` or `abs()` as appropriate

---

## 4. Human Approval & Memory Storage

### A. Post-Simulation UI Flow

```
Simulation Complete â†’ Show Results
                      â†“
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ Verdict Display   â”‚
              â”‚ - EXCELLENT/GOOD  â”‚
              â”‚ - Confidence: 87% â”‚
              â”‚ - Key Metrics     â”‚
              â”‚ - Recommendations â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ Human Decision    â”‚
              â”‚ âœ… Approve & Storeâ”‚
              â”‚ âŒ Discard        â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“
          Approved?    â”‚    Discarded?
               â†“YES    â”‚    â†“NO
          Store with   â”‚    Don't store
          metadata:    â”‚
          - human_approved: true
          - simulation_results
          - verdict & confidence
          - original query
```

**Code Location**: `app.py` lines 1545-1595

### B. Storage Structure

**What Gets Stored**:
```python
experiment_data = {
    # Design
    'title': "...",
    'description': "...",
    'components': [...],
    'beam_path': [...],
    'physics_explanation': "...",
    
    # Validation
    'simulation_results': {...},
    'verdict': 'EXCELLENT',
    'confidence': 0.87,
    
    # Human feedback
    'human_approved': True,
    'timestamp': "2025-11-19T...",
    
    # Context
    'user_query': "original user query",
    'conversation_context': [...]
}
```

**Storage Conditions**:
- User clicks "âœ… Approve & Store"
- Simulation completed successfully
- Verdict available (EXCELLENT/GOOD/ACCEPTABLE)
- Not already stored (deduplication check)

### C. Deduplication Logic

```python
# Check if already stored this design
already_stored = st.session_state.get(f"stored_{title}", False)

if already_stored:
    st.success("âœ… Already in knowledge base")
else:
    # Show approve/discard buttons
```

**Future Enhancement**: Semantic similarity check before storage
- Compare with existing experiments in memory
- If similarity > 0.95 â†’ Ask "This is very similar to [existing]. Still store?"

---

## 5. Retrieval & Reuse

### A. Similarity Search

**When User Submits Query**:
```python
similar = memory.retrieve_similar_experiments(query, n_results=1)
if similar and similar[0]['similarity_score'] > 0.85:
    # High match - return existing design
    return existing_design
```

**Similarity Calculation**:
- Semantic embedding via ChromaDB
- Cosine similarity of query embeddings
- Considers: title, description, physics, user query

### B. UI Presentation

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ“¦ Retrieved from Memory                â”‚
â”‚                                         â”‚
â”‚ This experiment was previously          â”‚
â”‚ designed and validated:                 â”‚
â”‚                                         â”‚
â”‚ â€¢ Similarity: 92% match                 â”‚
â”‚ â€¢ Original: "hong ou mandel setup"     â”‚
â”‚ â€¢ Status: âœ… Human-Approved            â”‚
â”‚ â€¢ Past Verdict: EXCELLENT (94%)        â”‚
â”‚                                         â”‚
â”‚         [ğŸ”„ Generate New Instead]       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**User Options**:
1. **Accept** - Use the retrieved design (immediate, no cost)
2. **Generate New** - Force fresh generation with memory as context

---

## 6. Complete Workflow Example

### Example: User asks "design hong ou mandel interference"

```
Step 1: Retrieval Check
â”œâ”€ Search memory for "hong ou mandel"
â”œâ”€ Found: "Hong-Ou-Mandel Interference Setup" (similarity: 0.91)
â”œâ”€ Status: Human-approved, EXCELLENT verdict
â””â”€ Action: Return existing design

Step 2: User sees retrieved design
â”œâ”€ Optical table diagram
â”œâ”€ "ğŸ“¦ Retrieved from Memory" banner
â”œâ”€ Metadata: 91% match, human-approved
â””â”€ Option: "ğŸ”„ Generate New"

Step 3a: User accepts retrieved design
â”œâ”€ No API calls
â”œâ”€ Can run simulation immediately
â”œâ”€ Can refine with chat questions
â””â”€ Design is production-ready

Step 3b: User clicks "Generate New"
â”œâ”€ Force regeneration flag set
â”œâ”€ Skip retrieval, proceed to generation
â”œâ”€ Use memory as context (not exact match)
â”œâ”€ LLM generates fresh design
â”œâ”€ Multi-cycle validation runs
â””â”€ New design presented

Step 4: Run Simulation (if new design)
â”œâ”€ Cycle 1: Generate code â†’ Review â†’ Execute
â”‚   â”œâ”€ Review found issues â†’ Revise
â”‚   â””â”€ Execute â†’ Success
â”œâ”€ Physics validation: All checks pass
â””â”€ Results: Visibility = 0.95, EXCELLENT

Step 5: Human Decision
â”œâ”€ User reviews: "This is better than stored version"
â”œâ”€ Clicks "âœ… Approve & Store"
â”œâ”€ System stores with human_approved=True
â””â”€ Next user will see THIS version (higher quality)

Result: Knowledge base grows with validated experiments
```

---

## 7. Configuration & Tuning

### Adjustable Parameters

#### Retrieval Threshold
```python
# llm_designer.py line 315
if similarity > 0.85:  # Adjust this threshold
```
- **0.95+**: Only exact matches (strict)
- **0.85-0.90**: High similarity (recommended)
- **0.75-0.85**: Moderate similarity (more retrieval)
- **< 0.75**: Low bar (may retrieve irrelevant)

#### Refinement Cycles
```python
# llm_designer.py init
self.max_refinement_cycles = 3  # Default: 3
```
- **1**: Fast but may miss errors
- **2**: Balanced
- **3**: Thorough (recommended)
- **5+**: Expensive, diminishing returns

#### Physics Validation Tolerances
```python
# simulation_agent.py _validate_physics()
if variance < -1e-10:  # Negative check
if entropy < -1e-10:   # Negative check
if purity < 0 or purity > 1.01:  # Range check
```

---

## 8. Monitoring & Debugging

### Key Log Messages

**Retrieval**:
```
ğŸ” Checking memory for existing similar experiments...
âœ… Found highly similar experiment (similarity: 0.92)
ğŸ“¦ Returning existing design
```

**Generation**:
```
ğŸ¤– Starting design for: [query]
ğŸ’¡ No directly relevant past work found - designing from scratch
âœ… Found relevant past work - using experience to enhance design!
```

**Simulation**:
```
ğŸ‘¨â€ğŸ”¬ Conducting physics code review...
âœ… Code passed physics review!
âš™ï¸  Executing simulation...
âš ï¸  Attempt 1 failed, retrying with error feedback...
âœ… Retry 2 successful with valid physics!
```

**Storage**:
```
ğŸ’¾ User approved design for storage
âœ… Stored as exp_1732046234.567
ğŸ“š Knowledge base: 25 experiments, 30 building blocks
```

### Debug Checks

**If designs not being retrieved**:
1. Check memory is enabled: `use_memory=True`
2. Verify experiments stored: `memory.get_statistics()`
3. Check similarity threshold (may be too high)
4. Inspect embedding quality: `memory.retrieve_similar_experiments(query, n=5)`

**If simulations always failing**:
1. Check QuTiP installed: `import qutip`
2. Review error messages in terminal
3. Check physics validation tolerances
4. Examine generated code in UI "View Simulation Code"

**If memory growing too large**:
1. Implement semantic deduplication before storage
2. Archive old experiments (timestamp-based)
3. Manually prune low-quality experiments
4. Adjust approval threshold

---

## 9. Future Enhancements

### Short Term
- [ ] Semantic deduplication check before storage
- [ ] User feedback comments on designs
- [ ] Design versioning (track iterations)
- [ ] Export/import knowledge base

### Medium Term
- [ ] Collaborative memory (share across users)
- [ ] Design ratings (5-star system)
- [ ] Automatic building block extraction improvements
- [ ] A/B testing of similar designs

### Long Term
- [ ] Active learning: Ask user clarifying questions
- [ ] Design optimization: Suggest parameter improvements
- [ ] Experiment planning: Multi-stage protocols
- [ ] Integration with lab equipment APIs

---

## Summary

The iterative refinement system provides:

âœ… **Retrieval-First** - Reuse validated experiments (zero cost, instant)  
âœ… **Multi-Cycle Generation** - Review â†’ Execute â†’ Fix â†’ Validate (3 cycles)  
âœ… **Human-in-the-Loop** - Expert approval before knowledge storage  
âœ… **Deduplication** - Avoid redundant similar experiments  
âœ… **Error-Driven Learning** - LLM learns from execution failures  
âœ… **Physics Validation** - Comprehensive checks on all metrics  

**Result**: A continuously improving AI that learns from validated successes and human expert feedback, building a curated knowledge base of proven quantum experiment designs.
